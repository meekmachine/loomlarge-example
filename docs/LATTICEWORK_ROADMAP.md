# Latticework Roadmap

Latticework is the collection of agencies under `src/latticework/` that listen to live microphone and webcam streams, turn those signals into verbal and nonverbal cues, and drive the avatar through the 3D engine bindings exposed by LoomLarge. Each folder represents one self-contained agency: it owns its inputs, runs its state machine, and emits either animation curves/snippets or audio/text instructions. LoomLarge’s job is simply to load the scene and expose the engine controls; all behavior logic lives inside these agencies.

## Architectural Facts

- **Agency Encapsulation**: Every behavior lives in a single agency. Examples: `transcription/` (mic → words), `conversation/` (dialog orchestration), `tts/` (text → speech + phonemes), `lipsync/` (phonemes → visemes), `prosodic/` (speech rhythm gestures), `eyeHeadTracking/` (webcam/mouse → gaze), `blink/` (autonomous blinks), `animation/` (applies curves/snippets to the host), `hair/` (specialized physics tweaks). Legacy versions live in `old_agencies/`.
- **Per-Agent Instancing**: Every agent/persona gets its own set of agencies. More humans simply introduce more mic/webcam streams; more avatars spin up parallel agency bundles. No shared singletons.
- **Sensor Ownership**: Agencies capture raw mic + webcam data themselves. Today this is browser APIs; soon we layer LiveKit so the same data flows through remote streams.
- **Animation Agency**: All motion on the avatar flows through the `animation` agency. It accepts curves generated by other agencies or pre-authored snippets and can retime/re-scale them (speed, intensity, playback direction) before applying them via the host engine bindings.
- **Outputs**: Agencies emit two classes of cues:
  - **Verbal**: audio buffers, phoneme timelines, transcribed text.
  - **Nonverbal**: animation curves/snippets for face/head/body, symbolic gestures (e.g., “raise brow”) that the animation agency turns into curves.

## Workstream 1 – Ground Truth + Completion

1. **Agency Registry**
   - Enumerate every folder under `src/latticework/`, noting its inputs, outputs, and whether it already follows the Service → Machine → Scheduler pattern.
   - Link to the relevant README inside each folder so future changes reference real files.
2. **Complete Trinity Pattern**
   - `transcription/`: add `transcriptionMachine` + scheduler so mic → text flow is deterministic regardless of browser or LiveKit source.
   - `tts/`: follow the architecture plan in `src/latticework/tts/TTS_AGENCY_ARCHITECTURE.md`; publish public API docs.
   - Verify `lipsync/`, `eyeHeadTracking/`, `prosodic/`, and `animation/` documents clearly state how they depend on host capabilities.
3. **Per-Agent Factory**
   - Build `createAgentLatticework(hostCaps, micSource, camSource)` that instantiates every agency for one avatar and connects sensor streams. ---  here we dont need to instantiate every agency on start up. we should be able to spin up these agencies on the fly. the hard part will be making the system work with multiple agents, so we will know which instance of latticework a given agent belongs to in a multi-agent setup.
   - Add tests to prove multiple agents + multiple humans can run simultaneously without shared state collisions. We dont need tests for this yet. We just need to make it work and independently test it. then this functionality will be covered by the same unit and functional testing we are using throughout the platform.
4. **Documentation Drafts**
   - README “chapters” for Project Setup (how LoomLarge exposes host capabilities) and Agency Pattern (how each folder works). Include file references so we can verify every statement.

## Workstream 2 – Signals, Persona Tuning, and LiveKit

1. **Signal Contracts**
   - Define structured messages (JSON schemas) for agent-to-agent influence (interrupts, empathy cues). Messages travel via LoomLarge events or LiveKit data channels. Agencies never mutate each other directly.
2. **Persona Weighting**
   - Extend persona configs so traits (e.g., from Cybernetic Big Five) drive agency parameters—how aggressively `prosodic` pulses, how `conversation` chooses strategies, etc.
   - Provide visualization tools to preview the effect of trait changes.
3. **LiveKit Sensor Support**
   - Wrap microphone/webcam acquisition so the same API can source local streams or LiveKit remote tracks. Document how each agency selects the source.
4. **Rapport & Strategic Agencies**
   - Promote the rapport and strategic interaction logic inside `conversation/` into standalone agencies (still per agent) so prosodic alignment and negotiation live behind clear APIs.
5. **Docs**
   - Chapters for Engines & Mapping (how animation agency talks to EngineThree/Babylon/VRoid/RPM) and Persona Systems (trait weights, activation).

## Workstream 3 – Multi-Agent / Multi-Human Coordination

1. **Adaptive Agency Graphs**
   - Support runtime toggles for each agent’s agencies (e.g., switch to “minimal gesture” profiles without muting speech). Provide policy presets (debate, interview).
2. **Memory & Knowledge**
   - Implement per-agent memory stores (vector + symbolic) with APIs that agencies call to fetch/commit context. No direct sharing; cross-agent info only flows via signals.
3. **Conflict & Negotiation Scripts**
   - Add programmable patterns inside `conversation/` (majority vote, weighted expertise) and ensure outputs remain curves/snippets/audio through the animation/TTS agencies. The goal is complex multi-agent discourse emerging from cooperating agencies, not individual agencies doing everything.
4. **Simulation Sandbox**
   - Headless runner that instantiates N agents + M humans, replays mic/webcam data, and records emitted cues for debugging. Include LiveKit-fed mic streams.
5. **Docs**
   - Chapters for Backend Orchestration (Python + LiveKit), Planner & Memory, Director/Synthesizer.

## Workstream 4 – Tooling & Ecosystem

1. **Agency Graph IDE**
   - Visual editor for configuring per-agent agency bundles, persona traits, and signal wiring. Exports config consumed by the per-agent factory.
2. **Monitoring & Metrics**
   - Dashboard per agent showing mic levels, transcription latency, viseme accuracy, prosodic density, gaze stability, etc.
3. **Plugin Ecosystem**
   - SDK for creating new agencies or swapping implementations, with contract tests ensuring they accept mic/webcam inputs and emit animation curves/audio through the expected APIs.
4. **Avatar Adapter Enhancements**
   - Expand animation mappings so `animation/` can drive VRoid, Ready Player Me, and other rigs with reduced geometry; include retargeting presets.
5. **Docs**
   - Chapter on Tooling & Sandbox plus appendices tying agencies to the referenced literature (Minsky, Emotion Machine, Lugrin & Pelachaud, Gumperz, Oxford Handbook of Facial Perception, CBFT).

## Success Metrics

- Every statement in documentation references actual files/README sections.
- All agencies follow the trinity pattern (or explicitly justify why not).
- Per-agent factory proves multiple agents/humans can run concurrently with isolated state.
- LiveKit integration lets agencies swap between local and remote sensor sources without code changes.
- Animation agency remains the sole path for applying curves/snippets, with tests verifying other agencies only emit data.

## “Virtual Humans” – Combined Chapter Plan

Single book that spans LoomLarge + Latticework so readers can move from host setup to individual agencies. We selected the following references because they collectively cover cognition (Minsky), affect/emotion (The Emotion Machine), socially interactive behavior (Lugrin & Pelachaud), discourse analysis (Gumperz), perceptual science (Oxford Handbook of Facial Perception), and trait-based personality modeling (Cybernetic Big Five Theory). Each chapter cites the specific sections relevant to the agency it describes.

1. **Chapter 0 – LoomLarge Project Setup** (repo layout, tooling, backend linkage). References: *Society of Mind* “Societies of Mind”, *Emotion Machine* “Resourcefulness”.
2. **Chapter 1 – LoomLarge UI Weave** (panels, debug overlays, persona selectors). References: Lugrin & Pelachaud interface chapters, Gumperz “Conversational Inference”.
3. **Chapter 2 – Engines & Mapping Lexicon** (EngineThree/Fiber/Babylon adapters, VRoid/RPM mappings). References: *Oxford Handbook* “Dynamic Facial Information”, Lugrin & Pelachaud embodiment chapters, CBFT traits.
4. **Chapter 3 – Persona & Interaction Systems** (persona schemas, agency hooks, activation weights). References: *Emotion Machine* “Critics/Selectors”, *Society of Mind* “K-Lines”, CBFT literature.
5. **Chapter 4 – Backend Orchestration & Remote Control** (Python backend, LiveKit streaming, remote directives). References: Gumperz “Contextualization Cues”, Lugrin & Pelachaud multimodal coordination.
6. **Chapter 5 – Perceptor Agencies** (transcription, eye/head tracking, blink). References: *Oxford Handbook* perception chapters, Lugrin & Pelachaud “Multimodal Perception”.
7. **Chapter 6 – Conversation Agency** (dialog management, signal routing). References: Gumperz discourse analysis, *Society of Mind* “Language Agents”.
8. **Chapter 7 – Rapport Agency** (prosody alignment, empathy cues). References: Lugrin & Pelachaud rapport chapters, *Emotion Machine* critic-selector patterns, CBFT Agreeableness/Extraversion.
9. **Chapter 8 – Strategic Interaction Agency** (negotiation, debate policies). References: *Society of Mind* “B- and C-brains”, *Emotion Machine* “Resourcefulness”, CBFT Conscientiousness/Openness.
10. **Chapter 9 – Planner & Memory Agencies** (episodic/vector memory, scratchpads). References: *Society of Mind* “K-Lines”, *Emotion Machine* “Reflective Thinking”.
11. **Chapter 10 – Director/Synthesizer & Animation Agency** (timeline blending, curve scheduling). References: *Society of Mind* master chapters, Lugrin & Pelachaud “Orchestrating Output”.
12. **Chapter 11 – Tooling, Sandbox, and Ecosystem** (agency IDE, monitoring, plugin SDK). References: Lugrin & Pelachaud evaluation methodologies, *Emotion Machine* “Common Sense”.

### Reference Rationale

- **Marvin Minsky – *Society of Mind***: Provides the core metaphor for agencies cooperating through simple rules; we mirror its structure when describing how each Latticework agency behaves.
- **Marvin Minsky – *The Emotion Machine***: Extends Society-of-Mind ideas into layered reasoning and emotional appraisal—useful for rapport, strategic interaction, and persona weighting.
- **Birgit Lugrin & Catherine Pelachaud – *Handbook on Socially Interactive Agents***: Offers practical guidelines for UI, embodiment, rapport, evaluation, and multimodal coordination that map directly to our agencies.
- **John Gumperz – *Discourse Strategies***: Supplies sociolinguistic insights (contextualization cues, conversational inference) needed to design the conversation, rapport, and strategic agencies.
- ***Oxford Handbook of Facial Perception***: Gives perceptual science backing for eye/head tracking, visemes, and facial realism—critical for the animation, prosodic, and perceptor agencies.
- **Cybernetic Big Five Theory (CBFT)**: Provides a trait-based framework for persona configuration and agency activation weights, ensuring our personality controls have psychological grounding.

Each chapter will cite the files under discussion (e.g., `src/latticework/animation/README.md`) so the book doubles as accurate technical documentation grounded in the actual code.
